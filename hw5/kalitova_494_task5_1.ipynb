{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate q-learning\n",
    "\n",
    "In this notebook you will teach a lasagne neural network to do Q-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Frameworks__ - we'll accept this homework in any deep learning framework. For example, it translates to TensorFlow almost line-to-line. However, we recommend you to stick to theano/lasagne unless you're certain about your skills in the framework of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: THEANO_FLAGS='floatX=float32'\n"
     ]
    }
   ],
   "source": [
    "%env THEANO_FLAGS='floatX=float32'\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-11-05 05:12:18,987] Making new env: LunarLander-v2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff627e05f60>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8JLUKANekhr1/wHgtcqa/2hvRgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff636f12ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v2\").env\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate (deep) Q-learning: building the network\n",
    "\n",
    "In this section we will build and train naive Q-learning with theano/lasagne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is initializing input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "#create input variables. We'll support multiple states at once\n",
    "\n",
    "\n",
    "current_states = T.matrix(\"states[batch,units]\")\n",
    "actions = T.ivector(\"action_ids[batch]\")\n",
    "rewards = T.vector(\"rewards[batch]\")\n",
    "next_states = T.matrix(\"next states[batch,units]\")\n",
    "is_end = T.ivector(\"vector[batch] where 1 means that session just ended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 8)\n"
     ]
    }
   ],
   "source": [
    "print((None,)+state_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "from lasagne.layers import *\n",
    "\n",
    "#input layer\n",
    "l_states = InputLayer((None,)+state_dim)\n",
    "\n",
    "\n",
    "# <Your architecture. Please start with a single-layer network>\n",
    "\n",
    "\n",
    "nn = DenseLayer(l_states, 200, nonlinearity=lasagne.nonlinearities.rectify)\n",
    "nn = DenseLayer(nn, 200, nonlinearity=lasagne.nonlinearities.rectify)\n",
    "\n",
    "\n",
    "#output layer\n",
    "l_qvalues = DenseLayer(nn,num_units=n_actions,nonlinearity=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Elemwise{add,no_inplace}.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasagne.layers.get_output(l_qvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting Q-values for `current_states`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get q-values for ALL actions in current_states\n",
    "predicted_qvalues = get_output(l_qvalues,{l_states:current_states})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compiling agent's \"GetQValues\" function\n",
    "get_qvalues = theano.function([current_states], T.argmax(predicted_qvalues, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#select q-values for chosen actions\n",
    "predicted_qvalues_for_actions = predicted_qvalues[T.arange(actions.shape[0]),actions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function and `update`\n",
    "Here we write a function similar to `agent.update`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#predict q-values for next states\n",
    "predicted_next_qvalues = get_output(l_qvalues, {l_states:next_states})\n",
    "\n",
    "#Computing target q-values under \n",
    "gamma = 0.99\n",
    "target_qvalues_for_actions = rewards + gamma * T.max(predicted_next_qvalues, axis=1)\n",
    "\n",
    "#zero-out q-values at the end\n",
    "target_qvalues_for_actions = (1-is_end)*target_qvalues_for_actions\n",
    "\n",
    "#don't compute gradient over target q-values (consider constant)\n",
    "target_qvalues_for_actions = theano.gradient.disconnected_grad(target_qvalues_for_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#mean squared error loss function\n",
    "loss = lasagne.objectives.squared_error(predicted_qvalues_for_actions, target_qvalues_for_actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#all network weights\n",
    "all_weights = get_all_params(l_qvalues,trainable=True)\n",
    "\n",
    "#network updates. Note the small learning rate (for stability)\n",
    "updates = lasagne.updates.adam(loss.mean(),all_weights,learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Training function that resembles agent.update(state,action,reward,next_state) \n",
    "#with 1 more argument meaning is_end\n",
    "train_step = theano.function([current_states,actions,rewards,next_states,is_end],\n",
    "                             updates=updates, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epsilon = 0.6 #initial epsilon\n",
    "\n",
    "def generate_session(t_max=1000):\n",
    "    \"\"\"play env with approximate q-learning agent and train it at the same time\"\"\"\n",
    "    \n",
    "    total_reward = 0\n",
    "    s = env.reset()\n",
    "    for t in range(t_max):\n",
    "        \n",
    "        #get action q-values from the network\n",
    "        q_values = get_qvalues(np.array([s],dtype=np.float32))[0] \n",
    "        rnd = np.random.uniform()\n",
    "        if rnd < epsilon:\n",
    "            a = np.random.choice(np.arange(n_actions))\n",
    "        else:\n",
    "            a = q_values\n",
    "        \n",
    "        new_s,r,done,info = env.step(a)\n",
    "        \n",
    "        #train agent one step. Note that we use one-element arrays instead of scalars \n",
    "        #because that's what function accepts.\n",
    "        train_step(np.array([s],dtype=np.float32),[a],[r],\n",
    "                   np.array([new_s],dtype=np.float32),[done])\n",
    "        \n",
    "        total_reward+=r\n",
    "        \n",
    "        s = new_s\n",
    "        if done: break\n",
    "            \n",
    "    return total_reward\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'tqdm' from '/home/nimloth/anaconda3/lib/python3.5/site-packages/tqdm/__init__.py'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import imp\n",
    "imp.reload(tqdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]\u001b[A\n",
      "Exception in thread Thread-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nimloth/anaconda3/lib/python3.5/threading.py\", line 914, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/nimloth/anaconda3/lib/python3.5/site-packages/tqdm/_tqdm.py\", line 144, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"/home/nimloth/anaconda3/lib/python3.5/_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n",
      " 10%|█         | 100/1000 [3:32:23<31:51:35, 127.44s/it, epsilon=0.362, mean_reward=5.23] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You Win!\n"
     ]
    }
   ],
   "source": [
    "t = tqdm.trange(1000)\n",
    "for i in t:\n",
    "    \n",
    "    rewards = [generate_session() for _ in range(100)] #generate new sessions\n",
    "    \n",
    "    epsilon*=0.995\n",
    "    t.set_postfix(mean_reward=np.mean(rewards), epsilon=epsilon)\n",
    "#     print (\"mean reward:%.3f\\tepsilon:%.5f\"%(np.mean(rewards),epsilon))\n",
    "\n",
    "    if np.mean(rewards) > 5:\n",
    "        print (\"You Win!\")\n",
    "        break\n",
    "        \n",
    "    assert epsilon!=0, \"Please explore environment\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# epsilon=0 #Don't forget to reset epsilon back to initial value if you want to go on training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#record sessions\n",
    "import gym.wrappers\n",
    "env = gym.wrappers.Monitor(env,directory=\"videos\",force=True)\n",
    "sessions = [generate_session() for _ in range(100)]\n",
    "env.close()\n",
    "#unwrap \n",
    "env = env.env.env\n",
    "#upload to gym\n",
    "#gym.upload(\"./videos/\",api_key=\"<your_api_key>\") #you'll need me later\n",
    "\n",
    "#Warning! If you keep seeing error that reads something like\"DoubleWrapError\",\n",
    "#run env=gym.make(\"CartPole-v0\");env.reset();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
